\documentclass[12pt]{article}

\usepackage{prsteele}
\usepackage{fullpage}

\title{Bayesian Curve Fitting and Reversible-Jump Markov Chain Monte
  Carlo}
\author{Patrick Steele}
\date{\today}

\newcommand{\thetak}{\theta^{(k)}}
\newcommand{\Thetak}{\Theta^{(k)}}

\begin{document}

\maketitle

\begin{abstract}
  We present a Bayesian curve fitting technique for modeling
  single-dimensional data. The technique leaves the number and
  location of knots as parameters to be determined by the data, and
  produces curves that have an arbitrary number of continuous
  derivatives. Additionally, we present a package for the R
  statistical language implementing this technique, and discuss
  computational results.
\end{abstract}

\section{Introduction}

A common problem in statistics is to determine the functional
relationship between explanatory data $X$ and response data
$Y$. Specifically, given bivariate data $(x_i, y_i)$, $i = 1, \ldots,
n$, we wish to estimate the form of $f$, where it is assumed that
\begin{equation*}
  y_i = f(x_i) + \epsilon_i,
\end{equation*}
where $\epsilon_i$ is an error term with zero mean. This problem is
known as \textit{curve fitting}, as the final result is finding a
curve $f(x)$, $x \in \dom X$ that minimizes the difference between
$y_i$ and $f(x_i)$ for all $i$ by some measure. This is a very general
problem, and approaches generally first make assumptions about the
form of $f$. For example, one could assume that $f$ is simply the
linear interpolation of the data; for $x_1 < x_2 < \ldots < x_n$, $f$
would have the form
\begin{equation*}
  f(x) = 
  \begin{cases}
    y_1 + \frac{y_2 - y_1}{x_2 - x_1} (x - x_1), & x_1 \le x < x_2 \\
    \qquad \vdots \\
    y_{n - 1} + \frac{y_n - y_{n - 1}}{x_n - x_{n - 1}} (x - x_{n -
      1}), & x_{n - 1} \le x \le x_n.
  \end{cases}
\end{equation*}
However, such an $f$ lacks many desirable properties, such as
simplicity and smoothness, and will have trouble when the true
functional form of the relationship has rapidly varying derivatives or
discontinuities.

A natural generalization of a linear interpolation is to assume that
$f$ is of the form
\begin{equation*}
  f(x) = \sum_{i = 0}^N a_i x^i
\end{equation*}
for some $N$; a fitting technique would then find appropriate $N$ and
$a_1, \ldots, a_n$. However, accurately modeling many data can require
$N$ to be quite large; additionally, since we optimize over the
coefficients globally, such a problem is computationally difficult
\cite{denison1998automatic}. To overcome this, one can introduce
piecewise-defined polynomials; by breaking the domain of the
explanatory variable into disjoint pieces, fitting a single low-order
polynomial over each piece yields an improved fit with smoother
behavior that is computationally achievable. However, one must still
decide what order polynomials to fit, and where and what size the
subintervals should be. Additionally, one can consider whether the
resulting curve will have smooth or non-smooth behavior at the
boundaries of the subintervals; for example, it might be desirable for
$f$ to have a number of continuous derivatives. In \cite{akima1970new}
and \cite{inoue1986least} cubic polynomials are used over regularly
spaced subintervals. In \cite{dimatteo2001bayesian} cubic polynomials
are used, but the number and location of such polynomials are left as
free parameters. This approach is generalized in
\cite{denison1998automatic}, where the number of location of
polynomials are free parameters as well as the degree and smoothness
of fit polynomials. We focus on the results of
\cite{denison1998automatic}, and provide an R package implementing the
algorithm given.

\section{Bayesian curve fitting}

We consider fitting some class of piecewise polynomials over
subintervals of the domain of the explanatory variable. We refer to
the borders of the subintervals as \textit{knots} or \textit{knot
  points}; if the intervals are $\halfopen{a_1, a_2}, \halfopen{a_2,
  a_3}, \ldots, \sbrace{a_{n - 1}, a_n}$, then $a_1, a_2, \ldots, a_n$
are knot points, while $a_2, a_3, \ldots, a_{n - 1}$ are
\textit{interior knot points}. Clearly both the location and number of
interior knot points will effect the quality of a fit. Increasing the
number of knot points allows for a closer fit, while decreasing the
number of knot points offers a simpler fit. Carefully choosing the
location of knot points can decrease the number of knots required to
get a good fit; for example, there is less of a need for many knot
points along a section of data that is nearly linear than along a
section that is rapidly varying. Since these choices depend on the
data, it is natural to allow the data to choose them. To accomplish
this, we consider a Bayesian curve fitting model, as in
\cite{denison1998automatic} and \cite{dimatteo2001bayesian}; we
present the work of \cite{denison1998automatic} in detail.

\subsection{Model}

Since we are fitting using piecewise polynomials, the assumed
relationship between the explanatory and response data can be
expressed as
\begin{equation*}
  \label{modeleq}
  y_i = f_{k, l, l_0}(x_i) + \epsilon_i = \sum_{n = 0}^l \beta_{n, 0}
  (x_i - r_0)^n_+ + \sum_{m = 1}^k \sum_{n = l_0}^l \beta_{n, m} (x_i
  - r_m)^n_+ + \epsilon_i, \quad i = 1, \ldots, n,
\end{equation*}
where $k$ is the number of interior knot points chosen, $l$ is the
maximum degree polynomial we fit, and the resulting curve has at least
$l_0 - 1$ continuous derivatives (as may be discontinuous if $l_0 =
0$), and $(x)_+^n$ is $x^n$ if $x \ge 0$, and zero otherwise. When $l
= l_0 = 3$, this is a cubic-spline fit
\cite{denison1998automatic}. Candidate knot points are the $N + 1$
endpoints of $N$ equal-sized disjoint subintervals of the domain; any
two knot points in a model must have at least $l$ (unselected)
candidate knot points between them.

With this relationship in mind, we create a Bayesian model where the
number and location of knot points are free parameters to be
determined by the data; the parameters $l$ and $l_0$ are chosen by the
user. We define $M_k$ to be the class of models with $k$ interior knot
points, $k = 0, 1, \ldots$. Then the parameters space for models in class
$M_k$ is $\thetak = (r_1, \ldots, r_k, \sigma^2)$, where $r_1 <
\ldots < r_k$ are the knot points. Then the joint distribution of the
model class parameter $k$, the data $y$, and the model parameters
$\thetak$ is
\begin{equation*}
  p\paren{k, y, \thetak} = p(k) p\paren{\thetak | k}
  p\paren{y | k, \thetak}.
\end{equation*}
To make inferences about the model parameters we will need to know the
joint posterior of $\paren{k, \thetak}$, given by
\begin{equation*}
  p\paren{k, \theta^{(k)} | y} = \frac{1}{Z} p(k) p\paren{\thetak | y}
  p\paren{y | k, \thetak},
\end{equation*}
where
\begin{equation*}
  Z = \sum_{k = 0}^\infty p(k) \cbrace{\int_{\Thetak} p\paren{\thetak
      | y} p\paren{y | k, \thetak} d \thetak}
\end{equation*}
and $\Thetak = \cbrace{\dom X}^k \times \real_+$ is the parameter
space of $M_k$. We assume that all error terms $\epsilon_i$ are drawn
from a normal distribution with zero mean and variance $\sigma^2$; the
prior on $\sigma^{-2}$ is the Gamma distribution with shape and scale
parameters of $10^{-3}$. This prior was chosen because, as a diffuse
prior, it makes few assumptions about he data as well as being
algorithmically convenient, which we will discuss later. The prior on
the model class is a Poisson distribution with mean $\lambda$, so
\begin{equation*}
  p(k) = \frac{e^{-\lambda} \lambda^k}{k!}, \quad k = 0, 1, \ldots.
\end{equation*}

Even limiting ourselves to models $M_0, M_1, \ldots,
M_{k_{\text{max}}}$, computing $Z$ is intractable. To overcome this,
Denison et al.\ \cite{denison1998automatic} use reversible-jump
Metropolis-Hastings techniques to sample from the joint posterior.
Denison et al.\ further note that this model could be made more
rigorous by assigning prior distributions to the $\beta_{i, j}$
coefficients in $f_{k, l, l_0}$ defined by
equation~\ref{modeleq}. However, this leads to an enormous increase in
computational complexity, as the number of parameters in the posterior
increases by $l + 1 + k (l - l_0 + 1)$; even limiting ourselves to
differentiable quadratic polynomials causes us to need to estimate
approximately $2 k$-dimensional integrals for models in class
$M_k$. Instead, Denison et al.\ fit the polynomial coefficients using
least-squares regression for a given collection of knot points.

\subsection{Reversible-jump Metropolis-Hastings algorithm}

Observe that the dimension of the problem varies linearly with $k$ as
we consider models in class $M_k$. To model the changing dimension of
the problem, Denison et al.\ introduce the notion of birth, death, and
move steps. A birth step is the addition of a knot to a model; a death
step is the deletion of a knot from a model; and a move step is the
replacement of one knot with another in a model. Birth and death steps
change the dimension of the model, while move steps facilitate
exploring models of a given class. The algorithm proposed is as
follows: at iteration $i$ in the algorithm,
\begin{enumerate}
\item 
  Select a birth, death, or move step with probabilities $b_k$, $d_k$,
  and $\eta_k$, respectively;

\item
  Perform the selected step;

\item
  Obtain the polynomial coefficients using least-squares regression;

\item
  Accept or reject the proposed model;

\item
  Update $\sigma^2$ using a Gibbs step;

\item
  Compute the mean-squared error of the model.
\end{enumerate}
The algorithm begins with an arbitrary model, and terminates when the
mean-squared error grows stable. We now discuss each aspect of
the algorithm in detail.

\subsubsection{Choosing an initial model}

Since Metropolis-Hastings algorithms should converge regardless of
starting parameters, a natural choice is to choose an initial $k_0$
with probability $p(k)$. However, the restriction that any two
candidate knot points must be at least $l$ apart presents an
interesting computational problem of selecting uniformly from all
models of class $M_{k_0}$. Since we wish to sample uniformly from such
models, it suffices to be able to count the number $x$ of such models and
index the models from 1 to $x$.

To accomplish this, I utilized dynamic programming techniques. Let
$w(k, N)$ be the number of models in with $k$ interior knots and $N$
candidate knot locations; then, $x = w(k_0, N)$. The insight to the
problem decomposition is as follows: either a model has a knot at the
first location, or not. Then $w(k_0, N) = w(k_0 - 1, N - 1 - l) +
w(k_0, N - 1)$. Since this recursion has only $(k_0 + 1) (N + 1)$
possible function signatures, using memoization allows us to compute
$w(k_0, N)$ in $\bigO(k_0 N)$-time and -space. All that is left is to
index the models. This is done by iteratively constructing a
model. Suppose the knot positions $r_1, \ldots, r_{k'}$ have been
selected; the next possible knot location is $r_{k'} + 1 + l$. Since
there are $w(k_0 - k' - 1, N - r_{k'} - 2(1 + l))$ models with this index
selected and $w(k_0 - k', N - r_{k'} - 1 - l)$ models with it not
selected, we simply include this knot with weights as given.

\subsubsection{Gibbs step}

The Gibbs step required to draw $\sigma^2$ is defined by the prior
distributions given and the data likelihood. From
\cite{denison1998automatic}, we have that the likelihood is
\begin{equation*}
  L(\thetak | y, k) = \exp\cbrace{- n \log \sigma - \frac{1}{2
      \sigma^2} \sum_{i = 1}^n (y_i - f_{k, l, l_0}(x_i))^2}.
\end{equation*}
Since the prior on $\sigma^{-2}$ is a $\text{Gamma}(10^{-3}, 10^{-3})$
distribution, we find that the conditional posterior of $\sigma^{-2}$
is a $\text{Gamma}\paren{10^{-3} + \frac{n}{2}, \paren{10^{-3} +
    \frac{1}{2} \sum_{i = 1}^n (y_i - f_{k, l, l_0}(x_i))^2}^{-1}}$
distribution.

\subsubsection{Least-squares regression}

Recall the form of our polynomial,
\begin{equation*}
  y_i = f_{k, l, l_0}(x_i) = \sum_{n = 0}^l \beta_{n, 0} (x_i - r_0)^n_+ +
  \sum_{m = 1}^k \sum_{n = l_0}^l \beta_{n, m} (x_i - r_m)^n_+, \quad
  i = 1, \ldots, n.
\end{equation*}
We wish to determine the $\beta_{i, j}$ using least-squares
regression. Although the dependence of the fit is polynomial in the
explanatory variable, it is linear in the parameters to be fit. Thus,
we can compute the coefficients using the multiple linear regression
\begin{equation*}
  y_i \sim \sum_{n = 0}^l x_{n, 0, i} + \sum_{m = 1}^k \sum_{n = l_0}^l
  x_{n, m, i},
\end{equation*}
where
\begin{equation*}
  x_{n, m, i} = 
  \begin{cases}
    0, & x_i < r_m \\
    (x_i - r_m)^n, & \text{otherwise}
  \end{cases}
\end{equation*}
for $i = 1, \ldots, n$.

\subsubsection{Halting condition}

The halting condition given by Denison et al.\ is to halt when the
mean-squared error of the model ``settles down'', with a specific
interpretation left to the reader. To implement this in practice, I
record a running average of the mean-squared errors over model
iterations, and compare it to the current mean-squared error. If the
running average is $e_r$ and the mean-squared error of the current
model is $e$, to halt I require that $\abs{e_r - e} / e_r \le
\text{MSE-relative}$, where MSE-relative is a parameter set by the
user. Additionally, I require that $e \le \text{MSE-absolute}$, where
MSE-absolute is a parameter set by the user. The relative error term
ensures that we do not terminate while the models are growing better
(or worse), while the absolute term tries to ensure that the algorithm
will not halt in a local minimum. This allows a user to accommodate
knowledge of the data during the fit; a user who expects the data to
be noisy might require a small relative error but large absolute
error, while a user who expects the data to be simple to fit could
require both small relative and absolute error.

\subsubsection{Birth, death, and move steps and acceptance
  probability}

Although not interesting algorithmically, we present the probabilities
of each step and acceptance given by Denison et al.\ for completeness.

We begin with the birth, death, and move step probabilities. In
general, we wish to move towards more probable models; however, doing
so can lead to getting caught in local extrema, and so we randomize
over these steps, weighting the probabilities according to the
relative probabilities of the old and new models. Specifically, for a
model with $k$ interior nodes the birth probability is
\begin{equation*}
  b_k = c \min\cbrace{1, p(k + 1) / p(k)},
\end{equation*}
the death probability is
\begin{equation*}
  d_k = c \min\cbrace{1, p(k - 1) / p(k)},
\end{equation*}
and the move probability is $\eta_k = 1 - b_k - d_k$. The parameter
$c$ is chosen from $(0, 1/2)$ to control the rate of mixing (Denison
et al., and I, choose $c = .4$); the range of $c$ is chosen so $b_k +
d_k + \eta_k = 1$. Observe that $b_k = c$ when a $k + 1$-knot model
has higher prior probability that a $k$-knot model, and is $c p(k + 1)
/ p(k)$ otherwise. Likewise, $d_k = c$ when a $k - 1$-knot model has
higher prior probability than a $k$-knot model, and $c p(k - 1) /
p(k)$ otherwise. For $k = 0$, we define $b_k = 1$ and $d_k = \eta_k =
0$.

We now consider the acceptance probabilities of each proposed
model. The acceptance probability for a model is
\begin{equation*}
  \alpha = \min\cbrace{1, \text{likelihood ratio} \cdot \text{prior
      ratio} \cdot \text{proposal ratio}}.
\end{equation*}
As shown in \cite{denison1998automatic}, the acceptance probability
after a birth step is
\begin{equation*}
  \alpha = \min\cbrace{1, L(m_p, m_c) \frac{n - Z(k)}{n}},
\end{equation*}
where
\begin{equation*}
  Z(k) = 2 (l + 1) + k (2 l + 1)
\end{equation*}
is the number of candidate knot locations, and $L(m_p, m_c)$ is the
likelihood ratio of the proposed model to the current model.
Likewise, the acceptance probability after a death step is
\begin{equation*}
  \alpha = \min\cbrace{1, L(m_p, m_c) \frac{n}{n - Z(k)}},
\end{equation*}
and the acceptance probability after a move step is
\begin{equation*}
  \alpha = \min\cbrace{1, L(m_p, m_c)},
\end{equation*}
since the prior ratio is 1 because both models have $k$ interior
knots, and the proposal ratio is 1 since all distributions of
$k$-knots are equally likely.

Observe that in all cases we move to a more likely model with
probability 1, while we move to a less probable model with some
positive probability; this ensures that there is a path of positive
probability from the current model to any other model.

\section{Computational results}

I have created an R package for 

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}

%%  LocalWords:  bivariate subintervals Denison et al memoization
%%  LocalWords:  indices
