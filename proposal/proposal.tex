\documentclass{article}

\usepackage{prsteele}
\usepackage{fullpage}

\title{Bayesian Curve Fitting and Reversible-Jump Markov chain Monte
  Carlo}
\author{Patrick Steele}
\date{\today}

\begin{document}

\maketitle

\noindent In curve fitting, one seeks to approximate a function with a
number of smooth curves; this can be used to smooth noisy data, and
has applications in parametric and non-parametric regression
\cite{denison1998automatic}. Specifically, given bivariate data $(x_i,
y_i)$, we wish to estimate the function form of $f$, where it is
assumed that
\begin{equation*}
  y_i = f(x_i) + \epsilon_i,
\end{equation*}
where $\epsilon_i$ is an error term with zero mean. Curve fitting
techniques include polynomial fitting, where $f$ is estimated using
polynomials, and spline fitting
\cite{hastie1990generalized}. Polynomial fitting suffers from
individual observations $(x_i, y_i)$ greatly effecting the estimate at
points distant from $x_i$, and has difficulty estimating curves with
oscillations.

Piecewise polynomial curve fitting involves estimating the curve using
a number low-order polynomial fit over disjoint regions of the curve;
the union of these disjoint regions is the domain of the estimated
function $f$. We define the points in the domain at the edge of each
region as \textit{knot points}. Regression can be carried out using a
fixed or variable number of knots, and the location of the knots may
also be fixed or free. In \cite{denison1998automatic} and
\cite{dimatteo2001bayesian}, both the location and number of knot
points are left as free parameters to be determined by the data.

We consider applying Bayesian techniques to solve the curve fitting
problem using piecewise polynomials. In \cite{denison1998automatic},
we treat the location and number of knot points as free parameters,
and assume the errors $\epsilon_i \sim N(0, \sigma^2)$, where
$\sigma^2$ is an unknown constant. We assume that the true curve is
drawn from the class of models $M_0, M_1, \ldots$, where $M_k$ is the
class of models with $k$ interior knots. By placing a prior on the
space of models, a model can be selected using the posterior
distribution. In \cite{dimatteo2001bayesian}, the same problem is
considered when the data are drawn from an exponential family. In
both, reversible-jump Markov chain Monte Carlo techniques are used to
sample the posterior.

I will present the problem of Bayesian curve fitting as discussed in
\cite{denison1998automatic} and \cite{dimatteo2001bayesian}, and will
present their results. I will go into detail about the reversible-jump
Markov chain Monte Carlo techniques used to sample from the posterior,
as traditional Markov chain Monte Carlo does not apply since the
dimensionality of the problems vary; these Markov chain Monte Carlo
methods are introduced by Green \cite{green1995reversible}, which will
be presented as well. I will implement the curve fitting algorithm
given in \cite{denison1998automatic} as an R package, and demonstrate
it on test data sets.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
